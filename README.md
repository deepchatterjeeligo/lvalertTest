# lvalertTest

The idea is to set up a modular library which allows us to simulate data corresponding to all stages of an event's life in GraceDb. Each bit of data is generated by a different class within several modules. These classes all must have "genSchedule" methods, which generate a schedule of Actions as defined in ~/lib/schedule.py. Then, all these schedules are stitched together (Schedules know how to add themselves together) and we have an iterable list of things (with associated wait times) that we should do to simulate a full event.

Note, simulating multiple evnets can be sewn together by simply adding their Schedules.

Also, we may not know which GraceID will be assigned to a particular event a priori (ie: at instantiation time). Therefore, we provide a simple class that can be shared between actions associated with a single event. The class contains a single protected attribute that must be accessed through getters and setters. Thus, when an entry is actually created in GraceDB, the object can be updated and the information will be available to all associated actions in the future.

--------------------------------------------------

Other Goals include:
  - setting up a ``replay'' utility that will allow us to generate lvalert messages corresponding to every entry in a GraceID's log and therefore simulate it as it happened repeatedly. We'll want to add in a cut-off parameter that stops the replay after we reach a certain log entry (specified by the log entry's index in GraceDb).
  - setting up a ``dummy'' GraceDb client which will mimic the behavior of the real GraceDb but will read/write from the local filesystem instead of communicating with a remote server. In this way, we only have to switch which GraceDb client we instantiate (the real one or the dummy one) when testing and the rest of the code should run without issue. This will let us extensively fire-proof our code without spamming servers at UWM.

--------------------------------------------------
--------------------------------------------------

I've set up an lvalert node specifically for testing purposes. Here are the pertinents:

    username : gdb_processor
    password : ***

    resource : lvalertMP-test

    node : lvalertMP-TestNode

I've also written a simple script to ping this node with basic messages

    lvalertMP_test

and a config file with which I can run lvalert_listenMP[simple]

    lvalert_listenMPsimple.ini (points to childConfig.ini)

as well as a config file for a more standard lvalert_listen process to confirm we heard the alerts.

    lvalert_listen.ini (delegates to confirmation.sh)

This should give me a good enough place to start hard-core developing and debugging.

--------------------------------------------------

this appears to be working, at least in principle. We need to do the following, however:

  - improve logging within multiprocessingChildSimple.py so that everything comes with a big time-stamp and so that I can trace everything
  - implement a few basic modules for a few basic checks. For example, start with iDQ FAP statements and see if you can get this to work as desired.
  - implement check-pointing so that the process can be revived "without loss." Checkpointing must actually occur within mpChildSimple.py, but the control arguments should be passed in through lvalert_listenMPsimple (specified in config file)

We also need to come up with a way to specify which "type" of process this is so that we load the appropriate parseAlert function and associated libraries. This is more of an organizational detail than a serious technical impediment.
  -- current solution is an option in childConfig which will tell the script which library to source for parseAlert()

--------------------------------------------------

To Do:

  - set up poisson event generator
    - mimic each type of event (ie: pipeline)
  - set up dummy messages for expected follow-up processes
  - may want to submit events to an instance of simDB rather than just sending alerts?
